{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT5HwwZnFHWG"
   },
   "source": [
    "# BERT Lab 3: Document Classification\n",
    "\n",
    "In this third lab, you will see how to fine-tune BERT for document classification using the Wikipedia Personal Attacks as an example.\n",
    "\n",
    "In the last lab, you have applied BERT to sentence classification. In this lab, you will look at longer pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cI2SVj-ZHuHP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnP_imlGFHWP"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wVAOFOrFHWT"
   },
   "source": [
    "As always, let's start with setting the correct device and installing the transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJGzrTU6FHWU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    assert False, \"Please select GPU in the Colab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9qtCcRdFHWa"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cS6FJ9_iFHWg"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "As stated earlier, you will use Wikipedia Personal Attacks. The dataset is a corpus of discussion comments from English Wikipedia talk pages. Comments are grouped into different files by year. The objective is to classify if the comments contain some personal attacks or not. So, like CoLA dataset, the task is a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHfpRDP_FHWh"
   },
   "source": [
    "First, download the dataset and then use `pandas` to parse the `.tsv` file, and use the BERT tokenizer to pre-process the input data for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TQ5Su33HuHc"
   },
   "outputs": [],
   "source": [
    "# Downloading the dataset\n",
    "import urllib.request as request\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./Wikipedia_Talk_Labels\"):\n",
    "    os.mkdir(\"./Wikipedia_Talk_Labels\")\n",
    "\n",
    "files = [\n",
    "    (\"./Wikipedia_Talk_Labels/attack_annotated_comments.tsv\", \"https://ndownloader.figshare.com/files/7554634\"),\n",
    "    (\"./Wikipedia_Talk_Labels/attack_annotations.tsv\", \"https://ndownloader.figshare.com/files/7554637\")\n",
    "]\n",
    "\n",
    "for (filename, url) in files:\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading\", filename)\n",
    "        request.urlretrieve(url, filename)\n",
    "        print(\"    Done ....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j78QN7XfHuHo"
   },
   "outputs": [],
   "source": [
    "# Parse file\n",
    "import pandas as pd\n",
    "print(\"Parsing the dataset .tsv file ...\")\n",
    "\n",
    "comments = pd.read_csv(files[0][0], sep=\"\\t\", index_col=0)\n",
    "annotations = pd.read_csv(files[1][0], sep=\"\\t\")\n",
    "\n",
    "dataset_sizes = comments[[\"comment\", \"split\"]].groupby(\"split\").count()\n",
    "print(\"Dataset Sizes: {}\".format(dataset_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5QWtklhFHWt"
   },
   "source": [
    "Let's see a few row to see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSfKlLwbFHWu"
   },
   "outputs": [],
   "source": [
    "# Display the first row of the table\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHOC9eU0FHWz"
   },
   "outputs": [],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udO8wFZOFHW2"
   },
   "source": [
    "We are provided with three splits, let's see how big are each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBSzn68gFHW3"
   },
   "outputs": [],
   "source": [
    "# See the sizes of each split\n",
    "comments[[\"comment\", \"split\"]].groupby('split').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYxG3Lg6FHW8"
   },
   "source": [
    "#### Labels\n",
    "\n",
    "The comments are uniquely identified by their 'rev_id'. The annotation table has multiple row for each comment because they are multiple labelers. You will consider a comment as an attack if the majority of the annotators agree that it is an attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evbqJsjRHuHx"
   },
   "outputs": [],
   "source": [
    "# A comment is an attack if the majority of the annotators agree\n",
    "labels = annotations.groupby(\"rev_id\")[\"attack\"].mean() > 0.5\n",
    "comments[\"attack\"] = labels\n",
    "\n",
    "# remove newline and tab tokens\n",
    "comments[\"comment\"] = comments[\"comment\"].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments[\"comment\"] = comments[\"comment\"].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "\n",
    "comments.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zARHhH-6FHXC"
   },
   "source": [
    "Now, let's divide the dataset into training and validation and testing comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aAfbac1uHuH5"
   },
   "outputs": [],
   "source": [
    "# Splits\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "val_comments = comments.query(\"split=='dev'\")\n",
    "test_comments = comments.query(\"split=='test'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-p-gMcvFHXH"
   },
   "source": [
    "Let's display some of the comments labeled as containing an attack. We'll use `textwrap` to wrap a single paragraph in text, and return a single string containing the wrapped paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a1N4RRDHuIA"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import random\n",
    "\n",
    "# Get some positive samples (comments with attacks)\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "attack_examples = train_comments.query(\"attack\")[\"comment\"]\n",
    "\n",
    "for i in range(10):\n",
    "    j = random.choice(attack_examples.index)\n",
    "    print(wrapper.fill(attack_examples[j]))\n",
    "    print(\" ---------- \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJRfH6GXFHXN"
   },
   "source": [
    "Some stats on the distribution of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wkux2sl3HuIJ"
   },
   "outputs": [],
   "source": [
    "total_comments = comments.shape[0]\n",
    "num_attacks = comments.query(\"attack\").shape[0]\n",
    "\n",
    "print(f\"Percentage of attack comments {(num_attacks / total_comments) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBDRRRuNFHXR"
   },
   "source": [
    "As we can see, this is a highly imbalanced classes, so predicting 0 will give us 88% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Avc4Yl78FHXS"
   },
   "outputs": [],
   "source": [
    "labels = train_comments.attack.to_numpy().astype(int)\n",
    "print(f\"Number of positive comments {labels.sum()} and negative ones {len(labels) - labels.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTK99cnBFHXX"
   },
   "source": [
    "## Tokenization & BERT input length limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1561wnrFHXX"
   },
   "source": [
    "Like discussed in the introduction, BERT has a maximum of 512 tokens. In this part of the lab, you will look at how this limitation can affect in practice and study some possible approaches to address this limitation.\n",
    "\n",
    "Let's start by seeing the distribution of lengths of the tokenized comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vM_i3FY_FHXY"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Avoid printing warnings\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Tokenizer the comments\n",
    "input_ids, lengths = [], []\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "print(\"Tokenizing comments ....\")\n",
    "input_ids = [tokenizer.encode(s, add_special_tokens=True) for s in tqdm(train_comments.comment)]\n",
    "lengths = [len(input_id) for input_id in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tN98_ax1FHXb"
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of comment lengths\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "# Consider all length > 512 as equal to 512, if not the distribution will be skewed\n",
    "lengths_shortened = [min(l, 512) for l in lengths]\n",
    "sns.distplot(lengths_shortened, kde=False, rug=False)\n",
    "\n",
    "plt.xlabel(\"Comments length\")\n",
    "plt.ylabel(\"# Comments\")\n",
    "plt.title(\"Comment lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0bAkXhgFHXf"
   },
   "source": [
    "#### Solutions:\n",
    "\n",
    "How can we solve this problem? There's no obvious solution. There are two simple and straightforward approaches that we can try:\n",
    "\n",
    "\n",
    "**Truncation:**\n",
    "The first solution is to simply drop some of the tokens, and hope that the remaining text is enough to predict the correct label.\n",
    "But what tokens have to be dropped? We can drop:\n",
    "- From the beginning.\n",
    "- At the end.\n",
    "- In this middle.\n",
    "- At a random starting position.\n",
    "\n",
    "In this [paper](https://arxiv.org/abs/1905.05583), the auhors experimented on IMDb movie dataset, and showed that keeping the first 125 tokens and the last 382 tokens is ok. The total number of tokens remaining is 512, leaving room for the two special tokens `[CLS]` and `[SEP]` that are neccessary to append for BERT fine-tunning.\n",
    "\n",
    "**Chunking:** \n",
    "The second solution is Chunking. It consists in dividing the test into 512-token chunks and generate embeddings for each of these chunks. These embeddings are then combined (like a simple summation or other pooling strategies) before performing the final classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jONnj9SG3U5"
   },
   "source": [
    "##  <span style=\"color:red\">Your turn. </span>\n",
    "We propose to use Truncation, and truncate all the inputs to max length of 128, similar to the second lab. You will need to do:\n",
    "\n",
    "1. **You first need the tokens if the lengh of the input sequence is > 128.**\n",
    "1. **Pad the sequences of length < 128.**\n",
    "1. **Create the attention masks to differentiate between the padding tokens and the rest of tokens.**\n",
    "1. **Split the data into train and validation: 90 %and 10% as in lab 2.**\n",
    "1. **Create the datasets and the dataloaders.**\n",
    "1. **Create the model and the optimizer using the same hyperparameters as lab 2, but only use 1 epoch.**\n",
    "1. **Fine-tune the BERT model for classification. Only train for one epoch, because the dataset is a big, and it will take approx. 30min.**\n",
    "1. **Apply the same preprocessing to the test data (tokenizer, truncate, create dataset and dataloader).**\n",
    "1. **Apply the model to the test data.**\n",
    "1. **Given how imbalanced the dataset is, compute ROC AUC metric using [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) on the test data. <span style=\"color:red\">The results should be > 0.95 </span>** \n",
    "\n",
    "Please use the second lab as a reference, the changes to be made are minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHqAuCPcHuJm"
   },
   "outputs": [],
   "source": [
    "# Your turn"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "document_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
