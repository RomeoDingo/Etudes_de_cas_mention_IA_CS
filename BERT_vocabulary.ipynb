{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='./Figs/cs-logo.png' width=200></center>\n",
    "\n",
    "\n",
    "\n",
    "<h6><center></center></h6>\n",
    "\n",
    "<h1>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Lab 1 : Introduction to BERT</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0wWu4oEKYtc"
   },
   "source": [
    "The first task is to install the [transformers](https://huggingface.co/transformers/) library. The others requirements (pytorch, numpy, matplotlib) are installed by default on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7181,
     "status": "ok",
     "timestamp": 1613517217600,
     "user": {
      "displayName": "Celine Hudelot",
      "photoUrl": "https://lh6.googleusercontent.com/-H6PRdXmlftg/AAAAAAAAAAI/AAAAAAAAKfM/OjR2ncBmL6k/s64/photo.jpg",
      "userId": "00470689273490527131"
     },
     "user_tz": -60
    },
    "id": "PzCjfNB6jksJ",
    "outputId": "2db2270e-ce39-43c0-c9e0-adc61da94b45"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwpkz_WYjscB"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cpr3fdTrP951"
   },
   "outputs": [],
   "source": [
    "# Set plotting style\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8FD0tDpKVBF"
   },
   "source": [
    "## Inspect BERT Vocabulary\n",
    "\n",
    "Let us begin by inspecing the BERT vocabulary that is the words, subwords and characters that embeddings are learned by BERT during pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2GhWrQYP957"
   },
   "source": [
    "### Vocabulary\n",
    "First, we'll retrieve the entire list of \"tokens\" and write these out to text files so we can see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRf7NXJGgsnG"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer, and write each token on a new line\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "with open(\"vocabulary.txt\", 'w') as f:\n",
    "    for token in tokenizer.vocab.keys():\n",
    "        f.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8geTQ3Iidsv"
   },
   "source": [
    "Now if you go and open the file we're just dumped, you'll see the vocabulary BERT uses, for example:\n",
    "\n",
    "* The first 999 tokens (1-indexed) appear to be reserved, and most are of the form [unused957].\n",
    "    * 1   - [PAD]\n",
    "    * 101 - [UNK]\n",
    "    * 102 - [CLS]\n",
    "    * 103 - [SEP]\n",
    "    * 104 - [MASK]\n",
    "* Rows 1000-1996 appear to be a dump of individual characters. \n",
    "    * They don't appear to be sorted by frequency (e.g., the letters of the alphabet are all in sequence).\n",
    "* The first word is \"the\" at position 1997.\n",
    "    * From there, the words appear to be sorted by frequency. \n",
    "    * The top ~18 words are whole words, and then number 2016 is ##s, the most common subword.\n",
    "    * The last whole word is at 29612, \"necessitated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uviYdpTZYMmd"
   },
   "source": [
    "### Single Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu-KbpbR1L4q"
   },
   "source": [
    "As discussed earlier, BERT vocabulary contains subwords and characters that are very useful to represent some input text that is not in the vocabulary. It avoids the need of UNKOWN tokens.\n",
    "\n",
    "Let's investigate how much of the vocabulary are single characters and subwords, i.e., subwords have a '##' as a prefix, so **##s** is a subword and **s** is a character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBYA-hHWP96F"
   },
   "source": [
    "\n",
    "The following code prints out all of the single character tokens in vocabulary, as well as all of the single-character tokens preceded by '##'.\n",
    "\n",
    "It turns out that these are matching sets--for every standalone character there is also a '##' version. There are 997 single character tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6ysF8LD1fbl"
   },
   "source": [
    "The following cell iterates over the vocabulary, pulling out all of the single character tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHpH2NEPy3t9"
   },
   "outputs": [],
   "source": [
    "# Fetch tokens that are either characters, so of length one\n",
    "# Or tokens that are either subword of one character, so of length 3 and a prefix ##\n",
    "\n",
    "one_chars = []\n",
    "one_chars_subwords = []\n",
    "\n",
    "for token in tokenizer.vocab.keys():\n",
    "    if len(token) == 1:\n",
    "        one_chars.append(token)\n",
    "    \n",
    "    elif len(token) == 3 and token[0:2] == '##':\n",
    "        one_chars_subwords.append(token)\n",
    "\n",
    "print('Number of single character tokens:', len(one_chars), '\\n')\n",
    "\n",
    "print('Number of single character subwords:', len(one_chars_subwords), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCzfsjHrzZYn"
   },
   "outputs": [],
   "source": [
    "# Print all of the single characters, 40 per row.\n",
    "for i in range(0, len(one_chars), 40):\n",
    "    print(' '.join(one_chars[i:i + 40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62MD-M1N0R-W"
   },
   "outputs": [],
   "source": [
    "# Print all of the single character subwords, 40 per row, without the hashes.\n",
    "one_chars_subwords = [token.replace('##', '') for token in one_chars_subwords]\n",
    "\n",
    "for i in range(0, len(one_chars_subwords), 40):\n",
    "    print(' '.join(one_chars_subwords[i:i + 40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-TFrVP9005S"
   },
   "outputs": [],
   "source": [
    "# We see that each character can also be a subword\n",
    "print('Are the two sets identical?', set(one_chars) == set(one_chars_subwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPnW2RetYc97"
   },
   "source": [
    "### Subwords vs. Whole-words\n",
    "\n",
    "Now, let's gather some statistics on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjvrWcNTdkAW"
   },
   "outputs": [],
   "source": [
    "# Measure the length of every token in the vocab.\n",
    "token_lengths = [len(token) for token in tokenizer.vocab.keys()]\n",
    "\n",
    "# Plot the number of tokens of each length.\n",
    "sns.countplot(token_lengths)\n",
    "plt.title('Vocab Token Lengths')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('# of Tokens')\n",
    "\n",
    "print('Maximum token length:', max(token_lengths))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQ8YUh75Usqj"
   },
   "source": [
    "##  <span style=\"color:red\">Your turn. </span>\n",
    "\n",
    "1. **Count the number of subwords and whole words in the vocabulary.**\n",
    "2. **Plot the lengths of the subwords and whole words.**\n",
    "3. **Percentage of subwords and whole words out of the whole vocabulary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69Vf46p-Ym97"
   },
   "outputs": [],
   "source": [
    "# Count the number of subwords in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGeB26JiHane"
   },
   "outputs": [],
   "source": [
    "# Plot the subword lengths (not including the two '##' characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJh6wl2bSzaC"
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of words that are '##' subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRaaspCfyqsY"
   },
   "source": [
    "### Names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Aldwa2EP96t"
   },
   "source": [
    "Let's see if BERT vocabulary contains any names. You will use a list of popular names provided by Gutenberg [here]('http://www.gutenberg.org/files/3201/files/NAMES.TXT'). So first, you will download it using wget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7OLeBs_zBiR"
   },
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "import wget\n",
    "\n",
    "url = 'http://www.gutenberg.org/files/3201/files/NAMES.TXT'\n",
    "wget.download(url, 'first-names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHF1JgrqY7Zi"
   },
   "outputs": [],
   "source": [
    "# Read and decode the names, then convert them to lowercase, and strip newlines.\n",
    "\n",
    "with open('first-names.txt', 'rb') as f:\n",
    "    names_encoded = f.readlines()\n",
    "\n",
    "names = []\n",
    "for name in names_encoded:\n",
    "    try:\n",
    "        names.append(name.rstrip().lower().decode('utf-8'))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print('Number of names: {:,}'.format(len(names)))\n",
    "print('Example:', random.choice(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gefw2nfP960"
   },
   "source": [
    "##  <span style=\"color:red\">Your turn.</span>\n",
    "\n",
    "1. **Count how many names are in the vocabulary.**\n",
    "2. **Count how many number are in the vocabulary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HGvwsy0ynkq"
   },
   "source": [
    "### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6heL05KQzmCa"
   },
   "outputs": [],
   "source": [
    "# Count the number of names in the vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEY1IHydP965"
   },
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5T_-gfQZKm6"
   },
   "outputs": [],
   "source": [
    "# Count how many numbers are in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_vocabulary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
